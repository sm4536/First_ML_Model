{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4A4ZbKtsfaj"
   },
   "source": [
    "# <H1>The DXC AI Starter</H1>\n",
    "\n",
    "The code in this document makes it easier to build and deploy a machine-learning microservice. It installs the required library dependencies, builds a data pipeline, builds a model, deploys a microservice, and publishes an API endpoint to the microservice. Find the code marked with <code># TODO</code> and replace it with your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDKBEfYM9d7t"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/dxc-technology/DXC-Industrialized-AI-Starter/blob/master/DXC_Industrialized_AI_Starter.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/dxc-technology/DXC-Industrialized-AI-Starter\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3XNqBz5s57T"
   },
   "source": [
    "## <H2> Set up the development environment</H2>\n",
    "\n",
    "This code installs all the packages you'll need. Run it first. It should take 30 seconds or so to complete. If you get missing module errors later, it may be because you haven't run this code. Restart the runtime/session after executing the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HGr3U_fhNIOk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "##After executing this code, You must restart the runtime/session to use newly installed versions.\n",
    "%%capture\n",
    "# ! pip install DXC-Industrialized-AI-Starter\n",
    "! pip install DXC-Industrialized-AI-Starter==2.0.7\n",
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRaLDS7aKbz4"
   },
   "source": [
    "From DXC-Industrialized-AI-Starter library import dxc-ai package\n",
    "\n",
    "### <b>Todo: </b>\n",
    "Restart the runtime/session after executing the above code cell. \n",
    "\n",
    "Runtime -> Restart Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KpvJo8WDKbz4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dxc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c35e8d157712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdxc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dxc'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "from dxc import ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6341FvyvhP8"
   },
   "source": [
    "## <H2>The Industrialized AI Open Badge Academy</H2>\n",
    "\n",
    "The AI Open Badges are verifiable, portable digital badges with embedded metadata about skills and achievements. They comply with the Open Badges Specification and are shareable across the web. This code defines the parameters needed to apply for an Industrialized AI Open Badge. This is where you define the email address that gets credit for the badge, the platform responsible for issuing the badge, and the evidence used to justify granting the badge. You should not have to change any of the badge platform parameters. For the badge evidence, you must paste a link to this notebook.\n",
    "<code>AI_Badge</code> is an enumeration of all unique badges. <code>ai_badge_id</code> is a mapping from <code>AI_Badge</code> to a unique identifier.<br />\n",
    "**AI_Badge:** <ol> <li> CREATE_DATA_STORIES </li><li>RUN_AGILE_TRANSFORMATION</li><li>BUILD_DATA_PIPELINES</li><li>RUN_AI_EXPERIMENT</li><li>BUILD_UTILITY_AI_SERVICES</li><li>PERFORM_AI_FORENSICS</li><li>TEST</li> </ol>**AI_Guild_Roles:** <ol><li>PROJECT_MANAGER</li><li>DATA_SCIENTIST</li><li>DATA_ENGINEER</li><li>ALL</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tbAi4NqRD3I"
   },
   "outputs": [],
   "source": [
    "# TODO: create an AI guild profile\n",
    "ai_guild_profile = {\n",
    "    \"guild_number\": 37,\n",
    "    #Provide the URL to the current notebook\n",
    "    \"badge_evidence\": \"https://colab.research.google.com/drive/18G3PKNxFJk-_h0S8unPPfywHAfakt3AR?usp=sharing\",\n",
    "    \"badge_platform_apiKey\": \"Yp8bmtzN85lrkGGmhjAM8jGpC1QniYw6EFk5lHh7\",\n",
    "    \"badge_platform_apiHost\": \"https://uefowgpyw6.execute-api.us-east-1.amazonaws.com/\",\n",
    "    \"badge_platform_apiBasePath\": \"prod/partner/\",\n",
    "    #Please identify guild members and roles\n",
    "    #Please have each guild member use their DXC email address\n",
    "    \"guild_members\" : {\n",
    "        1: {\n",
    "            \"badge_applicant_email\": \"smathari@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        2: {\n",
    "            \"badge_applicant_email\": \"sjoshi86@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        3: {\n",
    "            \"badge_applicant_email\": \"nvelagapudi2@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        4: {\n",
    "            \"badge_applicant_email\": \"sb263@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        5: {\n",
    "            \"badge_applicant_email\": \"dlakshminar2@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        6: {\n",
    "            \"badge_applicant_email\": \"rta3@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        },\n",
    "        7: {\n",
    "            \"badge_applicant_email\": \"rgupta329@dxc.com\",\n",
    "            \"roles\" : [ai.AI_Guild_Role.ALL]\n",
    "        } \n",
    "    }  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6JTRSGJv_YF"
   },
   "source": [
    "## Import Modules\n",
    "\n",
    "This code imports the modules that you will need from each installed library. If you require additional modules, place them here. Modules that have been depricated should be upgraded or replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dI7LxsdwfpaN"
   },
   "outputs": [],
   "source": [
    "import doctest #documenting data stories\n",
    "from IPython.display import YouTubeVideo\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hQY_JlNwM7x"
   },
   "source": [
    "## <H1>Create a Data Story</H1>\n",
    "\n",
    "The data story defines what the microservice is required to do. The code in the section accesses the raw data and defines an interface that the microservice must satisfy. Explore the raw data. Decide what the microservice will do. Write a test (data story) that will pass only when the microservice is successfully deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0vCfP-d1vQn"
   },
   "source": [
    "## <H2> Access the raw data </H2>\n",
    "Getting access to raw data is the very first task you have to complete. Your microservice is a wrapper for a machine-learning model. This code accesses the raw data that will be used to train the model.The read_data_frame_from_local_csv function allows you to import local character-delimited (commas, tabs, spaces) files.All parameters are optional. By default, the function will infer the header from the data, but an explicit header can be specified.read_data_frame_from_remote_csv works the same way except that it reads the file from a URL instead of from your local machine. The URL is required. The read_data_frame_from_local_excel_file function allows you to import XLSX files.The read_data_frame_from_local_json function allows you to import JSON files. When the file explorer is launched, you must select an XLSX file or the function will result in an error. The read_data_frame_from_remote_json function reads JSON files from a URL. the JSON data is flattened (in the case of nested data) and cast into Pandas data frame.\n",
    "\n",
    "NOTE: Run the below code to access required file format. For <code>remote files</code> provide the <code> URL</code>, in case of <code>local file</code> one you run the code, it will allow you to select the file from your local drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "RVixIH6Xwbwu",
    "outputId": "8b786b0e-c112-4a14-be03-0d487ff861d9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "6iceT_43tET8",
    "outputId": "aa104223-a720-4263-cba1-d3da81b1dda0"
   },
   "outputs": [],
   "source": [
    "# To upload dataset into google colab file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "7k2-8NoBH8ry",
    "outputId": "38d7ab08-85c1-4053-f752-70e0e7410b6a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Access raw data.\n",
    "##reads json from a url and flattens it into a dataframe\n",
    "# URL to download the file: https://www.kaggle.com/teertha/ushealthinsurancedataset \n",
    "# Dataset can be uploaded by using the above cell code\n",
    "import pandas as pd\n",
    "\n",
    "dataset =  pd.read_csv(\"insurance.csv\")\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iyuZOEW8uJ0x",
    "outputId": "1f65cddd-ca6d-427a-d11b-c490717f8628"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hSbyYGP199H"
   },
   "source": [
    "### <H2> Define data fields </H2>\n",
    "Mention the <code>text_fileds</code>,<code>date_fields</code>,<code>numeric_fields</code> and <code>categorical_fields</code> as per you data set. Below are example only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hIRPuq10Kb0V"
   },
   "outputs": [],
   "source": [
    "# TODO: define the data fields\n",
    "text_fields = []\n",
    "date_fields = []\n",
    "numeric_fields = ['age','bmi','charges','children']\n",
    "categorical_fields = ['sex','smoker','region']\n",
    "target_clm=['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdV-mB-z2F4s"
   },
   "source": [
    "### <H2> Clean the raw data </H2>\n",
    "Execute <code>raw_data</code> so that it accesses your raw data and returns it as a Pandas dataframe. Any preprocessing of the raw data should be done here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mEFE68czKb0X",
    "outputId": "65cf9015-104a-4153-a535-74d4fc9bbd2d"
   },
   "outputs": [],
   "source": [
    "#clean the data\n",
    "impute = True\n",
    "data1 = ai.clean_dataframe(dataset, impute, text_fields, date_fields, numeric_fields, categorical_fields)\n",
    "\n",
    "#display excerpts of the raw data\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN-TbCbb2P-m"
   },
   "source": [
    "### <H2>Explore the raw data</H2>\n",
    "Now that you've read in the raw data, you can explore the data to determine how it can be used. This code provides methods for visualizing the data in useful ways.\n",
    "\n",
    "<code>explore_features</code> visualizes the relationships between all features in a given data frame. Areas of heat show closely-related features. This visualization is useful when trying to determine which features can be predicted and which features are needed to make the prediction.\n",
    "\n",
    "<code>visualize_missing_data</code> creates a visual display of missing data in a data frame. Each column of the data frame is shown as a column in the graph. Missing data is represented as horizontal lines in each column. This visualization is useful when determining whether or not to impute missing values or for determining whether or not the data is complete enough for analysis.\n",
    "\n",
    "<code>plot_distributions</code> creates a distribution graph for each column in a given data frame. Graphs for data types that cannot be plotted on a distribution graph without refinement (types like dates), will show as blank in the output. This visualization is useful for determining skew or bias in the source data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDLnkhkwUjiU"
   },
   "source": [
    "Use <code>visualize_missing_data</code> to visualize missing fields in your raw data. Determine if imputing is necessary. Refine <code>raw_data()</code>, if necessary, and repeat this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "9qIwNHFzUQ05",
    "outputId": "57888d49-17a2-48b5-b790-cb52ef4f036d"
   },
   "outputs": [],
   "source": [
    "ai.visualize_missing_data(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "OUgy7Hc4UJO7",
    "outputId": "79a576e5-459b-4747-8007-ef19937df933"
   },
   "outputs": [],
   "source": [
    "# checking for null values in the dataset\n",
    "data1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "c4_JwPEgUTrV",
    "outputId": "581259a4-3be8-465b-e730-4f112f4ff7b0"
   },
   "outputs": [],
   "source": [
    "# checking for null values in the dataset\n",
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGiAQVC5Ulyv"
   },
   "source": [
    "Use <code>explore_features</code> to explore the correlations between features in the <code>raw_data</code>. Use the visualization to form a hypothesis about how the <code>raw_data</code> can be used. It may be necessary to enrich <code>raw_data</code> with other features to increase the number and strength of correlations. If necessary, refine <code>raw_data()</code> and repeat this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "5EBDrxFQUXWM",
    "outputId": "0958a2b4-fc37-44b2-c8b3-bcccd44913ea"
   },
   "outputs": [],
   "source": [
    "ai.explore_features(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDaM6Us3DqKK"
   },
   "source": [
    "Use <code>plot_distributions</code> to show the distributions for each feature in <code>raw_data</code>. Depending on <code>raw_data</code>, this visualization may take several minutes to complete. Use the visualization to determine if there is a data skew that may prevent proper analysis or useful insight. If necessary, refine <code>raw_data()</code> and repeat this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 804
    },
    "id": "Wuh20Fg2Dsno",
    "outputId": "2b300c1a-98fd-4846-8be0-12f03d303583"
   },
   "outputs": [],
   "source": [
    "ai.plot_distributions(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO1cBTFo3sk9"
   },
   "source": [
    "## <H2>Define a story</H2>\n",
    "\n",
    "The data story is a unit test that will only pass when the microservice is successfully deployed. After defining <code>raw_data()</code>, you will build a <code>data_story()</code>. Although this test will fail (initially), it defines the requirements for all remaining work. After writing the <code>data_story()</code>, complete the remaining tasks in this notebook. Rerun the <code>data_story()</code>. At this point, the test should succeed. All tasks are successfully complete when the <code>data_story()</code> succeeds.\n",
    "\n",
    "<b>DO NOT SKIP THIS STEP.</b> Although unit testing does not contribute to the functionality that you will deploy, it does determine the requirements of success. You should clearly document your goals before continuing. This video provides an overview of test-driven development. It describes the concept of writing tests first and the reasons for doing so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "fa7O3HsTyGVU",
    "outputId": "eb7cc19b-e5c3-4fdc-fb8c-6cc6a35fb636",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('uGaNkTahrIw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jxQds4_UCeC"
   },
   "source": [
    "This video provides an overview of Python Doctests. It provides an explanation of automated testing in Python. It walks you through the basic tasks of creating and executing a test. Watch this video if you are unfamiliar with Doctests. This video should be removed or replaced if data stories are executed using something other than Doctests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwdAwrAJT28G",
    "outputId": "e6bcb775-ec8a-4689-f4f2-c660c206fea8"
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('_BFeAJ8hC7Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaC0JpxPVESi"
   },
   "source": [
    "This code defines a unit test that sends data to an API endpoint and checks for an expected result. Update the <code>Context</code>, <code>Intent</code>, and <code>Design</code> to reflect your story. \n",
    "\n",
    "The <code>design</code> is the specification for your AI microservice. It defines the URL enpoint for the service. The test submits test input to the endpoint and tests if the output is within an expected range. Given the input you defined, you must also define an expected range within which the microservice will output when it is working properly. This means that you must form an expectation or reasonable behavior for the microservice.\n",
    "\n",
    "The <code>datastory</code> function acts as a contract that automatically verifies when you have completed the microservice. Create the <code>datastory()</code> and verify that the test fails. Complete the remaining tasks in this notebook. Rerun the <code>datastory()</code> and verify that the test passes. If the requirements of the microservice changes, update <code>datastory</code> and repeat this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "fcx54s92p1et",
    "outputId": "76ee9d4d-ec42-4bea-f3c3-acf7f05276f9"
   },
   "outputs": [],
   "source": [
    "# TODO: write the AI microservice specification\n",
    "def datastory(api_endpoint, input, header):\n",
    "    \"\"\"\n",
    "    Context:\n",
    "    \n",
    "    This microservice is part of AI which predicts the Insurance premium based on the factors of: \n",
    "     - Smoking\n",
    "     - Region\n",
    "     - Age\n",
    "     - Gender\n",
    "     - BMI\n",
    "    =================================================\n",
    "    \n",
    "    Intent:\n",
    "    \n",
    "    This microservice intends to provide information regarding change of premiums for an individual based on the age, BMI and smoking habits while also considering \n",
    "    at the same time the other factors such as gender/region/dependents.\n",
    "\n",
    "    This microservice can be used by the Insurance Companies to predict the premium rates for an individual based on age, BMI and smoking habits. When this microservice is \n",
    "    integrated with any Insurance Company portal, customer could check how much premiuim is to be paid by providing the inputs as mentioned in the DESIGN section below.\n",
    "    ========================================================\n",
    "    \n",
    "    Data Source: \n",
    "    \n",
    "    Data set has been taken from Kaggle: https://www.kaggle.com/teertha/ushealthinsurancedataset\n",
    "    This dataset contains 1338 rows of insured data, where the Insurance charges are given against the following attributes of the insured: \n",
    "    Age, Sex, BMI, Number of Children, Smoker and Region. The attributes are a mix of numeric and categorical variables.\n",
    "\n",
    "    Numerical attributes:\n",
    "     - age\n",
    "     - bmi\n",
    "     - children\n",
    "    \n",
    "    Categorical attributes:\n",
    "     - sex\n",
    "     - smoker\n",
    "     - region\n",
    "\n",
    "    Target attribute:\n",
    "     - charges\n",
    "\n",
    "    ========================================================\n",
    "    \n",
    "    Observations on dataset: \n",
    "\n",
    "    We have observed that there is strong positive correlation between \"smoker\" and \"charges\" attribute and no correlation between 'region' and 'charges' attributes. Positive \n",
    "    correlation implies, the attributes are dependent on each other and no correlation would imply that the change on 'region' attribute will not be effecting the 'charges'\n",
    "    attribute. Our dataset does not contain any negatively correlated attributes. \n",
    "    \n",
    "    As per the observation from our dataset, 'age', 'smoker', 'bmi' and 'sex' attributes are majorly contributing towards variation of 'charges' attribute. \n",
    "    After exploring the data we found out that there are no missing, null or undefined values in the dataset.\n",
    "    ========================================================\n",
    "\n",
    "    Business Value:\n",
    "    \n",
    "    Once this micro service is consumed and made available to the front end customers, it would allow the individual to reach and look out for the premium themselves which gives a \n",
    "    better experience to them. Also, the need for external agents to perform this task can be automated which will allow the company to channel some premium discount to the customer. \n",
    "    As the customer experience is the key in the market and insurance premium are always cut-throat in the market, such transparency will give customer an experience and command \n",
    "    that will help improve the business in the longer run.\n",
    "\n",
    "    ========================================================\n",
    "\n",
    "    Doctest working: \n",
    "    \n",
    "    The inputs required for our microservice are 'age', 'sex', 'bmi', 'children', 'smoker' and 'region'. When the inputs are provided, the microservice will predict the charges of \n",
    "    Insurance premium for the individual. If any of the input is not provided, there are chances that our microservice may fail to execute. \n",
    "    \n",
    "    Our aim is that our microservice would provide 95% of accuracy in predicting the Insurance premium.\n",
    "\n",
    "\n",
    "    Design:\n",
    "    >>> api_endpoint = \"https://api.algorithmia.com/v1/algo/shubhamjoshi/aibootcamp/0.1.0\"\n",
    "    >>> input = '{\"age\":19, \"sex\":\"female\", \"bmi\":27.9, \"children\":0, \"smoker\":\"yes\", \"region\":\"southwest\"}'\n",
    "    >>> header = {'Content-Type': 'application/json',  'Authorization': 'simVnAbGo/j8B2nfYqb5yxKRw891'}\n",
    "    >>> 9000 < float(datastory(api_endpoint, input, header)) < 9200\n",
    "    \n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "      headers = {\n",
    "          'Content-Type': 'application/json',\n",
    "          'Authorization': 'simVnAbGo/j8B2nfYqb5yxKRw891',\n",
    "      }\n",
    "      params = (\n",
    "          ('timeout', '300'),\n",
    "      )\n",
    "      data = input\n",
    "      response = requests.post(api_endpoint, headers=headers, params=params, data=data)\n",
    "      result = response.json()['result']['results']\n",
    "    \n",
    "    except Exception as error:\n",
    "      result = {error}\n",
    "\n",
    "    return result\n",
    "\n",
    "doctest.testmod(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd_LreMO4zIL"
   },
   "source": [
    "### <H2>Apply for the Create Data Stories badge </H2>\n",
    "This code applies for the Create Data Stories Industrialized AI Open Badge.<code>apply_for_an_ai_badge</code> applies for a a secific <code>ai_badge</code> on behalf of the user specified in the <code>ai_guide_profile</code>.\n",
    "<b>Run this code only if you are interested in earning the badge.</b> This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that:\n",
    "<ul>\n",
    "  <li>You have successfuly completed all <code>#TODO</code> items for <code>datastory()</code>.</li>\n",
    "  <li><code>datastory()</code> is a test that runs and fails.\n",
    "  <li>The <code>datastory()</code> makes sense given the output of <code>raw_data()</code>.</li>\n",
    "</ul>\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes.</li>\n",
    "\n",
    "### <H2>Todo: </H2>\n",
    "Before applying for the create data stories badge please answer the following questions.\n",
    "\n",
    "- Please provide the reviewer access to the raw data so that the reviewer can upload the data and run the code block.\n",
    "- Describe your observation and analysis of the data exploration? \n",
    "- Describe how will you implement the AI functionality/AI driven transformation in your project?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKVXbRWtkMIZ"
   },
   "source": [
    "# **Todo: Response**\n",
    "\n",
    "*Please provide the reviewer access to the raw data so that the reviewer can upload the data and run the code block.*\n",
    "\n",
    "**Ans**: Access provided to the reviewer.\n",
    "\n",
    "*Describe your observation and analysis of the data exploration?*\n",
    "\n",
    "**Ans**: The data set taken is based on regression analysis. We have observed that there is strong positive correlation between \"smoker\" and \"charges\" attribute and no correlation between 'region' and 'charges' attributes. Positive correlation implies, the attributes are dependent on each other and no correlation would imply that the change on 'region' attribute will not be effecting the 'charges' attribute. Our dataset does not contain any negatively correlated attributes. As per the observation from our dataset, 'age', 'smoker', 'bmi' and 'sex' attributes are majorly contributing towards variation of 'charges' attribute. After exploring the data we found out that there are no missing, null or undefined values in the dataset.\n",
    "\n",
    "*Describe how will you implement the AI functionality/AI driven transformation in your project?*\n",
    "\n",
    "**Ans**: We will deploy the data in MongoDb. Once done, we will use the data in the DXC-AI Starter package and deploy it in algorithmia. We will use the API to test the micro services. Once this micro service is consumed and made available to the front end customers, it would allow the individual to reach and look out for the premium themselves which gives a better experience to them. Also, the need for external agents to perform this task can be automated which will allow the company to channel some premium discount to the customer. As the customer experience is the key in the market and Insurance premium are always cut-throat, such transparency will give customer an experience that will help improve the business in the longer run.\n",
    "\n",
    "Doctest working: \n",
    "The inputs required for our microservice are 'age', 'sex', 'bmi', 'children', 'smoker' and 'region'. When the inputs are provided, the microservice will predict the charges of Insurance premium for the individual. If any of the input is not provided, there are chances that our microservice may fail to execute. Our aim is that our microservice would provide 95% of accuracy in predicting the Insurance premium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-yfZUWWravYN",
    "outputId": "052360d2-4064-41c8-e95e-48c8c6d975ba"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "# ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.CREATE_DATA_STORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEtGjglz5B21"
   },
   "source": [
    "## <H1>Build a data pipeline</H1>\n",
    "A data pipeline takes raw data and turns it into refined data that can be used to train and score a machine-learning model. The code in this section takes the output of <code>raw_data()</code> and puts it into a data store. It instructs the data store to refine the raw data into training data. It extracts the training data for use in training a machine-learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEM6xL7AU9Pk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v828RXGYTVOI"
   },
   "source": [
    "You will be using MongoDb as your data store. This video provides a general overview of MongoDB. The document model of MongoDB breaks from the traditional relational model of common relational databases. This video describes the basic idea behind the document mdoel. It also describes MongoDb clusters and the methods used to scale. It introduces MongoDB Atlas, which you will be using in the remainder of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOzG66BES_3I",
    "outputId": "acbcb2b6-c3d7-4b65-d4ad-c3892759e538"
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('EE8ZTQxa0AM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G-628DvDDeA"
   },
   "source": [
    "This video provides an overview of Mongo DB Atlas. It provides an explanation of the software. It walks you through the basic tasks of setting up an account and generating the proper connection credentials. Watch this video if you are unfamiliar with Mongo DB Atlas. This video should be removed or replaced if the data is stored using something other than Mongo DB Atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHaxkEA7bhwe",
    "outputId": "6ac7b40e-f1c4-4138-9e5e-4acc6aa689cb"
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('rPqRyYJmx2g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcUV1lMB5sTK"
   },
   "source": [
    "### <H2>Collect raw data</H2>\n",
    "\n",
    "This code defines the meta-data needed to connect to Mongo DB Atlas and create a new data store cluster. This is where you define basic information about the location of the cluster and the collection and database to use. Update this code with information appropriate to your project. This code assumes that the data store is Mongo DB Atlas. If the raw data is stored and refined using something other than Mongo DB Atlas, the parameters of the <code>data_layer</code> will need to be updated or replaced with something else. In order to provide the information required in <code>data_layer</code>, you must:\n",
    "<ul>\n",
    "  <li>Create a MongoDB Atlas account</li>\n",
    "  <li>Create a cluster</li>\n",
    "  <li>Create a user</li>\n",
    "  <li>Generate a connection string</li>\n",
    "</ul>\n",
    "\n",
    "Note: \n",
    "\n",
    "When you configure the IP whitelist for your cluster, choose to allow a connection from anywhere. Since your notebook is running in Colab, we cannot guarantee a known IP address.\n",
    "\n",
    "When creating the database connection string, choose the <code>Python</code> driver version 3.4 or later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUs-B1kMQx5q"
   },
   "outputs": [],
   "source": [
    "# TODO: specify the details of the data layer\n",
    "\n",
    "data_layer = {\n",
    "    \"connection_string\": \"mongodb+srv://shubham:Dxc1234@cluster0.cptej.mongodb.net/<dbname>?retryWrites=true&w=majority\",\n",
    "    \"collection_name\": \"ai_clnme_37\",\n",
    "    \"database_name\": \"ai_dbnme_37\",\n",
    "    \"data_source\":\"data1\",\n",
    "    \"cleaner\":\"no\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXEBM5Utfnsg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG_V01KuiOyV"
   },
   "source": [
    "Use <code>write_raw_data</code> function from <code>ai library</code>  to convert <code>Arrow</code> dates to <code>Strings</code> data types. This function also connects to Mongo DB ATlas a build a database and collection according to the parameters of <code>data_layers</code>.It also transfers the output of <code>raw_data()</code> into the database and collection. This function handles Mongo DB Atlas automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "VCqFr5UuTbRL",
    "outputId": "0fc334fc-e08f-4fd9-f075-d4ec2fcf1dc3"
   },
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "ZqVWcr41hBik",
    "outputId": "cf0ba107-56d1-43bf-e9f3-f3a4fb50fae2"
   },
   "outputs": [],
   "source": [
    "data1.age.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36JHHtMGpW3q"
   },
   "outputs": [],
   "source": [
    "data2 = ai.write_raw_data(data_layer, data1, date_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "yXlmhE2js3Iq",
    "outputId": "0853ab2e-4455-44ed-9135-950f25cbeb1b"
   },
   "outputs": [],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpoIXI0y5_9I"
   },
   "source": [
    "### <H2>Ingest and clean data</H2>\n",
    "This video provides an overview of how to create aggregation pipelines in Mongo DB Atlas. It describes the basic concepts and walks you through example pipelines. Watch this video if you are unfamiliar with Mongo DB Atlas aggregation pipelines. This video should be removed or replaced if the data is stored using something other than Mongo DB Atlas or the data is refined using something other than aggregation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXLbjn6p-2hE",
    "outputId": "4fbc8c0b-0f7e-49b3-f390-ca9ac184bc9c"
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('Kk6Er0c7srU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHhQtuvR-9D0"
   },
   "source": [
    "This code instructs the data store on how to refine the output of <code>raw_data()</code> into something that can be used to train a machine-learning model. Update <code>data_pipeline()</code> with code with an aggregation pipeline that fits your project. The refined data using <code>access_data_from_pipeline</code> from <code>ai library</code>  will be stored in the <code>df</code> Pandas dataframe. Make sure the output is what you want before continuing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "wYyNGWHNvcyw",
    "outputId": "2427d4f7-1626-4276-cded-53d620e0b5a1"
   },
   "outputs": [],
   "source": [
    "# TODO: define the code needed to refine the raw data\n",
    "\n",
    "def data_pipeline():\n",
    " \n",
    "  pipe = [\n",
    "          {\n",
    "              '$group':{\n",
    "                  '_id': {\n",
    "                            \"age\":\"$age_ins\",\n",
    "                            \"sex\":\"$sex\",\n",
    "                            \"bmi\":\"$bmi\",\n",
    "                            \"children\":\"$children\",\n",
    "                            \"smoker\":\"$smoker\",\n",
    "                            \"region\":\"$region\",\n",
    "                            \"charges\":\"$charges\",\n",
    "                  }\n",
    "              }\n",
    "          }\n",
    "  ]\n",
    "\n",
    "  return pipe\n",
    "\n",
    "df = ai.access_data_from_pipeline(data2, data_pipeline())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eRih0Vm6PCT"
   },
   "source": [
    "### <H2>Apply for the Build Data Pipeline badge</H2>\n",
    "\n",
    "This code applies for the Build Data Pipeline AI Open Badge.\n",
    "<b>Run this code only if you are interested in earning the badge.</b> This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that:\n",
    "<ul>\n",
    "  <li>You have successfuly completed all <code>#TODO</code> items for <code>datapieline()</code> and <code>access_data_from_pipeline()</code>.</li>\n",
    "  <li><code>df</code> is populated with data.</li>\n",
    "  <li>The data in <code>df</code> matches the input specified in the design section of <code>datastory()</code>.</li>\n",
    "</ul>\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bh2B-eI4cqyd",
    "outputId": "934b3252-e403-466f-eb7a-bf9b56b286cb"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.BUILD_DATA_PIPELINES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqKJ70WX6YNz"
   },
   "source": [
    "## <H1>Run an experiment</H1>\n",
    "An experiment trains and tests a machine-learning model. The code in this section runs a model through a complete lifecycle and saves the final model to the local drive. Run the code that defines a machine-learning model and its lifecycle. Design an experiment and execute it. Most of the work of choosing features and specific model parameters will be done automatically. The code will also automatically score each option and return the options with the best predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrPz7bzl6ebA"
   },
   "source": [
    "### <H2>Execute the experiment</H2>\n",
    "\n",
    "This code executes an experiment by running <code>run_experiment</code> from <code>ai library</code> on a model. Update <code>experiment_design</code> with parameters that fit your project. The <code>data</code> parameter should remain <code>df</code>-- the refined training data. The <code>model</code> parameter must be a <code>model</code> subclass. The <code>labels</code> parameter indicates the column of the <code>data</code> dataframe to be predicted. For the <code>prediction</code> model, the <code>meta-data</code> must describe the column to be predicted and the types for non-numeric columns. Check out [auto_ml](https://auto-ml.readthedocs.io/en/latest/index.html) to learn more about the auto_ml library usage and documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uj6maZHkaXi"
   },
   "source": [
    "Auto_Clustering model is also available in the AI_Starter, which inturn looks for the best model in the three Clustering models(Affinity Propagation, DBScan and K-means). Please refer to the [example document](https://github.com/dxc-technology/DXC-Industrialized-AI-Starter/blob/master/Examples/Clustering.ipynb.ipynb) on implementing the clustering model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "WFJNqmyqwnD3",
    "outputId": "d6cb08bb-8277-4486-dffc-7307b0d41bfc"
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjgGqLx9yURB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "XJfEZDRaHGY9",
    "outputId": "1f2c0efd-3ffa-4357-94a2-9c1962b55285"
   },
   "outputs": [],
   "source": [
    "# TODO: design and run an experiment\n",
    "experiment_design = {\n",
    "    #model options include ['regression()', 'classification()','timeseries']\n",
    "    \"model\": ai.regression(),\n",
    "    \"labels\": df['_id.charges'],\n",
    "    \"data\": df,\n",
    "    #Tell the model which column is 'output'\n",
    "    #Also note columns that aren't purely numerical\n",
    "    #Examples include ['nlp', 'date', 'categorical', 'ignore']\n",
    "    \"meta_data\": {\n",
    "      \"_id.charges\": \"output\",\n",
    "      \"_id.sex\": \"categorical\",\n",
    "      \"_id.smoker\": \"categorical\",\n",
    "      \"_id.region\": \"categorical\",\n",
    "\n",
    "  }\n",
    "}\n",
    "trained_model = ai.run_experiment(experiment_design, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6qANHXL6r79"
   },
   "source": [
    "### <H2> Apply for the Run AI Experiment badge </H2>\n",
    "\n",
    "This code applies for the Run AI Experiment Industrialized AI Open Badge. Run this code only if you are interested in earning the badge. This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that:\n",
    "<ul>\n",
    "<li>You have successfuly completed all <code>#TODO</code> items for the <code>experiment_design</code>.</li>\n",
    "<li>You have successfully executed <code>run_experiment</code> on the <code>experiment_design</code>.</li>\n",
    "</ul>\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes.</li>\n",
    "\n",
    "### <H2>Todo: </H2>\n",
    "Before applying for the Run AI Experiment badge please answer the following questions.\n",
    "<ul>\n",
    "<li><b>Goal:</b> What is the overall goal of the AI? This should be an expansion on the text already supplied for AI service in your Data Story. </li>\n",
    "\n",
    "<b>Ans</b>Overall goal of the AI is to predict the charges of insurance based on the smoking habits, age and sex.\n",
    "\n",
    "<li><b>Source:</b> This should be where and type of data was obtained. </li>\n",
    "<b>Ans</b> https://www.kaggle.com/teertha/ushealthinsurancedataset\n",
    "\n",
    "\n",
    "<li><b>Processing Steps:</b> Bullet points detailing what your AI intends to perform.</li>\n",
    "<b>Ans</b> It checks the smoking habit of the person for which insurance charge is to be obtained. Then it looks for the age and sex for the person. This way it is able to calculate for the premium charges of the person.\n",
    "\n",
    "<li><b>Output:</b> Describe the output type (last step from processing) and the type of resultants you expect to obtain (success and failures if appropriate).\n",
    "\n",
    "<b>Ans</b> Output type is continuous value, it provides the imsurance charges for the person applying for it. We estimate the charges to be within range and experiment to be successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MO76Riazc11C",
    "outputId": "ee49ef1e-c799-493c-8083-00bd9c804568"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.RUN_AI_EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPUEQQnxmc4h"
   },
   "source": [
    "## <H1>Model Explainability</H1>\n",
    "\n",
    "<b>Note:</b> In the present version, Model Explainability supports only the custom models. We are implementing the explainability changes for Auto_ml models. Changes will be published soon. \n",
    "\n",
    "Model explainability is one of the most important problems in machine learning today. The code in this section helps you to understand the output of the machine learning model using interactive dashboards. Model explainability supports [SHAP - based explainer](https://github.com/slundberg/shap). Depending on the model, Model Explainer uses one of the supported SHAP explainers.  \n",
    "\n",
    "### <H4>SHAP explainers:</H4>\n",
    "<ul>\n",
    "<li>SHAP TreeExplainer</li>\n",
    "<li>SHAP DeepExplainer</li>\n",
    "<li>SHAP LinearExplainer</li>\n",
    "<li>SHAP KernelExplainer</li></ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1S0uPKkmoAL"
   },
   "source": [
    "<code>ai.Global_Model_Explanation</code> function generates the overall model predictions and generates a dictionary of sorted feature importance names and values. <code>ai.Explanation_Dashboard</code> function will generate an interactive visualization dashboard, you can investigate different aspects of your dataset and trained model via four tab views:\n",
    "<ul>\n",
    "<li>Model Performance</li>\n",
    "<li>Data Explorer</li>\n",
    "<li>Aggregate Feature Importance</li>\n",
    "<li>Individual Feature Importance</li></ul>\n",
    "\n",
    "To generate the model explainability, you need to pass your model, training data, test data to the functions. You can also optionally pass in feature names and output class names(classification) which will be used to make the explanations and visualizations more informative. Explanations will be generated default on the test data. If you pass the value of <code>explantion_data</code> parameter as 'Training', then the explanation will be generated on training data. But with more examples, explanations will take longer although they may be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ukd1gScmtEW"
   },
   "source": [
    "Check out [Examples](https://github.com/dxc-technology/DXC-Industrialized-AI-Starter/tree/master/Examples) to understand how to use each function, what parameters are expected for each function. Also check out [shap](https://github.com/slundberg/shap), [lime](https://github.com/marcotcr/lime), [interpret-community](https://github.com/interpretml/interpret-community) libraries to learn more about the Model explainability and its usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcl6XqdXya2n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "cat_attribs = ['_id.sex','_id.smoker','_id.region']\n",
    "feature_names = df.columns\n",
    "feature_names = feature_names.to_numpy()\n",
    "feature_names\n",
    " \n",
    "full_pipeline = ColumnTransformer([\n",
    "        \n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "\n",
    " \n",
    "train_prepared = df.drop('_id.charges', axis=1)\n",
    "y_train1 = df['_id.charges']\n",
    " \n",
    "x_train1 = full_pipeline.fit_transform(train_prepared)\n",
    " \n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train1, y_train1, test_size=0.2, random_state=0)\n",
    " \n",
    "reg = GradientBoostingRegressor(n_estimators=100, max_depth=4,\n",
    "                                learning_rate=0.1, loss='huber',\n",
    "                                random_state=1)\n",
    "model = reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "IokhTHSKm1Hq",
    "outputId": "33e3ce23-99cb-4b8d-c348-108a4ab7ba98"
   },
   "outputs": [],
   "source": [
    "global_explanation = ai.Global_Model_Explanation(model,x_train,x_test,feature_names = None,classes = None, explantion_data = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "35bYrVzHm1kU",
    "outputId": "d22f7de2-95dc-49a1-c9ce-95b3bd43266d"
   },
   "outputs": [],
   "source": [
    "ai.Explanation_Dashboard(global_explanation, model, x_train, x_test, explantion_data = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SAhavLg65tw"
   },
   "source": [
    "## <H2>Apply for the AI Forensics AI badge</H2>\n",
    "\n",
    "This code applies for the AI Forensics Open Badge. Run this code only if you are interested in earning the badge. This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that:\n",
    "<ul>\n",
    "<li>After exploring the raw data, you have ensured that the <code>raw_data</code> is free from bias that could adversely affect the <code>intent</code> of the <code>datastory()</code></li>\n",
    "</ul>\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes.</li>\n",
    "\n",
    "### <H2>Todo: </H2>\n",
    "Before applying for the badge please answer the following questions.\n",
    "<ul>\n",
    "<li>State where you got the dataset and what it contains.</li>\n",
    "<b>Ans</b> We found the data from Kaggle: https://www.kaggle.com/teertha/ushealthinsurancedataset \n",
    "\n",
    "<li>State how you got your pipeline into your system.</li>\n",
    "<b>Ans</b>\n",
    "First the AI checks the parameters such assmoking, sex and age as the paarameter and then uses them to predict the insurance charges. \n",
    "\n",
    "Pipelining is done in MongoDB\n",
    "<li>Describe the Predicted Outcome  what you intended it to do.\n",
    "</li> <b>Ans</b> Outcome is the continuous variable which are the charges that incur to the person applying for insurance., These details can be used in pre-application process.\n",
    "\n",
    "<li>Analysis of the solution/General Analysis of your AI. This should be evaluation of the topics e.g. what topic modelling did you perform, topic identification,AI Experiment Result Observations. </li>\n",
    "<b>Ans</b> We have used DXC industrilised Auto ML program to model the data, we also used GradientBoostingRegressor for the explainability as the explainer was unable to explain the Auto MLmodel.\n",
    "<li>High Level Overview of your AI  enterprise risks, establish there is no risk.</li><b>Ans</b> DXC Industrialized AI tests different models and predictsthe outcomes based on these models. There are no risk seen for now.\n",
    "\n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5baY2RSkdfXo",
    "outputId": "6777055e-70ca-4e16-b599-b09a92bc1a7c"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.PERFORM_AI_FORENSICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os1PjyhQ7WfS"
   },
   "source": [
    "## <H1>Generate insight</H1>\n",
    "Insights are delivered through microservices with published APIs. The code in this section prepares an execution environment for the microservice, builds a microservice using the machine-learning model, deploys the microservice into the execution environment, and publishes an API enpoint for the microservice. Design the microservice and deploy it. The work of creating the microservice and deploying it will be done automatically. The code will also automatically handle the source code reposity management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8I56TIEb4w7y"
   },
   "source": [
    "This video provides an overview of the algorithm execution environment provided by Algorithmia. It describes the basic concept of the Algorithmia AI Layer and walks you through publishing a microservice. Watch this video if you are unfamiliar with publishing microservices using Algorithmia. This video should be removed or replaced if the microservices are run using something other than Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur3Ob3_5lVJb",
    "outputId": "59c99ab6-2a73-48dc-db58-dbf5d4f9d5f3"
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('56yt2Bouq0o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV0TELgF75sN"
   },
   "source": [
    "### <H2>Configure the microservice execution environment</H2>\n",
    "The execution environment is where the micorservice runs. This code assumes that the microservice execution environment is Algorithmia. If the microservices will be deployed somewhere other than Algorithmia, the code in this section will need to be replaced. In order to provide the information required to design the microservice, you must:\n",
    "<ul>\n",
    "  <li>create an Algorithmia account</li>\n",
    "  <li>create an <a href='https://algorithmia.com/user#credentials' target='new'>API key</a> with BOTH \"Read & Write Data\" and \"Manage Algorithms\" permissions enabled</li>\n",
    "  <li>create an algorithm user name</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiQEbhcO8GUs"
   },
   "source": [
    "### <H2> Design the microservice </H2>\n",
    "This code defines the parameters needed to build and delpoy a microservice based on the trained <code>model</code>. Update <code>microservice_design</code> with parameters appropriate for your project. The parameters must contain valid keys, namespaces, and model paths from Algorithmia (see above). The <code>microservice_design</code> will need to be updated if the microservice will run in something other than Algorithmia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34OjUSIpn6fG"
   },
   "outputs": [],
   "source": [
    "# TODO design a microservice\n",
    "microservice_design = {\n",
    "    \"microservice_name\": \"aibootcamp\",\n",
    "    \"microservice_description\": \"API generated for AIbootcamp for Insurance data\",\n",
    "    \"execution_environment_username\": \"shubhamjoshi\",\n",
    "    \"api_key\": \"simEKjTH3G4gK+6sdj9+sfKcjki1\",\n",
    "    \"api_namespace\": \"shubhamjoshi/aibootcamp\",   \n",
    "    \"model_path\":\"data://.my/mycollection\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caIljTPz8QkX"
   },
   "source": [
    "### <H2>Apply for the Create Utility AI Services Badge</H2>\n",
    "\n",
    "This code applies for the Create Utility AI Services Open Badge. Run this code only if you are interested in earning the badge. This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that you have successfully publised the AI microservice.\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes.</li>\n",
    "### <H2>Todo: </H2> \n",
    "Before applying for the badge please answer the following questions.\n",
    "<ul>\n",
    "<li>How did you monitor the AI Utility and manage the data pipelines? Should they have implemented any monitoring? In scope, was there additional needs?\n",
    "\n",
    "<b>Ans</b> Pipelines in the project have been done using MongoDB, and they have a security checklist which enable Access Control and Enforce Authentication.Also we have configured Role-Based Access Control\n",
    "</li><li>How did you manage the security of the data and models?\n",
    "\n",
    "<b>Ans</b> Once the Trained models are created they have been uploaded to algorithmia API which saves the trained model and provides access to the API which can be accessed using a json call. We wiill in return recieve a response with the prediction.\n",
    "</li><li>How did you expose models of the AI Utility as APIs?\n",
    "\n",
    "<b>Ans</b> We have used DXC Industrialized AI which automatically test different models using the installation of packages done in the collab notebook.\n",
    "</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v3bUdut2jIZX",
    "outputId": "48e0698a-0140-4146-c196-da9b44083dd9"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.BUILD_UTILITY_AI_SERVICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5ASIaeU8Ymx"
   },
   "source": [
    "### <H2>Publish the microservice</H2>\n",
    "<code>publish_microservice</code> function from <code>ai library</code> committs the changes made to the local, cloned GitHub repository and compiles the new microservice in Algorithmia and publish the microservice. It also generates the api endpoint for the newly published microservice. Run the code. Copy the URL and paste it into the <code>datastory</code>. After pasting the enpoint into the <code>datastory</code>, the <code>datastory</code> should succeed and you should be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "q1YxGTpeQKsv",
    "outputId": "378e84bc-8e07-4f48-e9c7-bc9f8a31c464"
   },
   "outputs": [],
   "source": [
    "# publish the micro service and display the url of the api\n",
    "api_url = ai.publish_microservice(microservice_design, trained_model, verbose = False)\n",
    "print(\"api url: \" + api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LQkS9Bi8mIk"
   },
   "source": [
    "## <H2>Apply for the Agile Transformation Badge</H2>\n",
    "This code applies for the Agile Transformation Open Badge. Run this code only if you are interested in earning the badge. This code will submit a link to this notebook to reviewers as evidence for your badge. Badge reviewers will inspect this notebook to ensure that the <code>datastory()</code> runs properly and the test passes.\n",
    "\n",
    "After inspection, you will receive notification either confirming that you have earned the badge or with suggested changes.\n",
    "### <H2>Todo: </H2> \n",
    "Before applying for the badge please answer the following questions.\n",
    "<ul><li>Explain your data, how it is transmitted and stored  how? \n",
    "\n",
    "<b>Ans</b> Our data consists of data related to Insurance charges for the individual applying for the insurance policy. We would use the data to predict the charges that would apply to the individual based on smoking habits, age and sex.\n",
    "\n",
    "Data was first accessed via csv and we wrangled the data as required and then the processed data is stored in mongoDB and accessed from there.\n",
    "\n",
    "Modelling is done using the DXCindustrilised AI with AutoML models and the accuracy was measured.\n",
    "\n",
    "We have then moved the model to algorithmia to create an APi which can be called to get the predictions.\n",
    "\n",
    "</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oz0IteesdrFX",
    "outputId": "83b9ab5d-f390-4423-cc3c-860f1a4e4c0f"
   },
   "outputs": [],
   "source": [
    "##Todo: When you are ready to apply for the badge, please uncomment the below code and run the code for badge submission.\n",
    "ai.apply_for_an_ai_badge(ai_guild_profile, ai.AI_Badge.RUN_AGILE_TRANSFORMATION)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Team37_Shubham_Main_Guild 37 DXC-Industrialized-AI-Starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
